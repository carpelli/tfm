{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding NeurIPS to the path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruben import PersistenceDiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 20:11:56.780323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "import math\n",
    "from gtda.diagrams import PersistenceEntropy, NumberOfPoints, Amplitude, PersistenceImage, ComplexPolynomial, BettiCurve\n",
    "from functools import partial \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import random\n",
    "# from model.Google.MutualInformationGoogle import get_mutual_information\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "def get_gen_gaps_folder(task_number):\n",
    "    return os.path.abspath('../../NeurIPSSoftware/results/task{}/generalization_gap'.format(task_number))\n",
    "def get_pds_folder(task_number):\n",
    "    return os.path.abspath('../../out/task{}/pds'.format(task_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_generalization_gap(model_file, folder_generalization):\n",
    "    with open('{}/{}'.format(folder_generalization, model_file), 'rb') as file:\n",
    "        gen_gap = pickle.load(file)\n",
    "    return gen_gap\n",
    "\n",
    "def load_generalization_gaps(task_number):\n",
    "    # folder_path = get_gen_gaps_folder(task_number)\n",
    "    # model_names_w_ext = os.listdir(folder_path)\n",
    "    # model_names = map(lambda name: name[:-4], model_names_w_ext)\n",
    "    # gen_gaps = map(lambda model_name: load_generalization_gap(model_name, folder_path), model_names_w_ext)\n",
    "    # return dict(zip(model_names, gen_gaps))\n",
    "    ref_file = Path(\n",
    "        \"/Users/otis/Documents/rubens_speelhoekje/google/google_data/public_data/reference_data/task1_v4/model_configs.json\")\n",
    "    return {\n",
    "        'model_' + k: v['metrics']['train_acc'] - v['metrics']['test_acc']\n",
    "        for k, v in json.loads(ref_file.read_text()).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outdir = Path('../out/task1/random')\n",
    "\n",
    "def load_pd(path):\n",
    "    # with open('{}/{}'.format(folder_pd, model_file), 'rb') as file:\n",
    "    #     pd = pickle.loads(file.read().replace(b'model.PersistenceDiagram',b'model.homology.PersistenceDiagram'))\n",
    "    # return pd\n",
    "    with open(path, 'rb') as f:\n",
    "        # return pickle.loads(f.read().replace(b'model.PersistenceDiagram',b'model.homology.PersistenceDiagram'))\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_pds(task_number):\n",
    "    # folder_path = get_pds_folder(task_number)\n",
    "    # pds_tries = os.listdir(folder_path)\n",
    "    # pds_by_model_number = dict()\n",
    "    # for pd_try in pds_tries:\n",
    "    #     try_path = '{}/{}'.format(folder_path, pd_try)\n",
    "    #     model_names_w_ext = os.listdir(try_path)\n",
    "    #     for model_name_w_ext in model_names_w_ext:\n",
    "    #         model_name = model_name_w_ext[:-4]\n",
    "    #         pd = load_pd(model_name_w_ext, try_path)\n",
    "    #         if model_name not in pds_by_model_number:\n",
    "    #             pds_by_model_number[model_name] = [pd]\n",
    "    #         else:\n",
    "    #             pds_by_model_number[model_name].append(pd)\n",
    "    # return pds_by_model_number\n",
    "    assert task_number == 1\n",
    "    return {p.stem: [load_pd(p)] for p in outdir.glob('model_*.bin')}\n",
    "\n",
    "def get_pds_and_generalization_gaps(task_number):\n",
    "    pds = load_pds(task_number)\n",
    "    gen_gaps = load_generalization_gaps(task_number)\n",
    "    model_names = pds.keys()\n",
    "    gen_gaps_and_pds = dict()\n",
    "    for model_name in model_names:\n",
    "        gen_gaps_and_pds[model_name] = (gen_gaps[model_name], pds[model_name])\n",
    "    return list(gen_gaps_and_pds.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model_158',\n",
       "  (0.2593799829483032, [<ruben.PersistenceDiagram at 0x138330070>])),\n",
       " ('model_602',\n",
       "  (0.30239999294281006, [<ruben.PersistenceDiagram at 0x104051610>])),\n",
       " ('model_212',\n",
       "  (0.25919997692108154, [<ruben.PersistenceDiagram at 0x11b846a60>])),\n",
       " ('model_213',\n",
       "  (0.2612000107765198, [<ruben.PersistenceDiagram at 0x138330ac0>])),\n",
       " ('model_603',\n",
       "  (0.3065999746322632, [<ruben.PersistenceDiagram at 0x1382f7580>])),\n",
       " ('model_159',\n",
       "  (0.2656399607658386, [<ruben.PersistenceDiagram at 0x13833b2b0>])),\n",
       " ('model_601',\n",
       "  (0.23843997716903687, [<ruben.PersistenceDiagram at 0x1382a38e0>])),\n",
       " ('model_600',\n",
       "  (0.23054003715515137, [<ruben.PersistenceDiagram at 0x13835c040>])),\n",
       " ('model_604',\n",
       "  (0.17900002002716064, [<ruben.PersistenceDiagram at 0x13816c190>])),\n",
       " ('model_599',\n",
       "  (0.20836001634597778, [<ruben.PersistenceDiagram at 0x138162fd0>])),\n",
       " ('model_214',\n",
       "  (0.2841799855232239, [<ruben.PersistenceDiagram at 0x1381eb700>])),\n",
       " ('model_215',\n",
       "  (0.28255999088287354, [<ruben.PersistenceDiagram at 0x138435cd0>])),\n",
       " ('model_598',\n",
       "  (0.2165200114250183, [<ruben.PersistenceDiagram at 0x138450fd0>])),\n",
       " ('model_605',\n",
       "  (0.1578199863433838, [<ruben.PersistenceDiagram at 0x1384967c0>])),\n",
       " ('model_607',\n",
       "  (0.17698001861572266, [<ruben.PersistenceDiagram at 0x13853b400>])),\n",
       " ('model_149',\n",
       "  (0.25936001539230347, [<ruben.PersistenceDiagram at 0x13b110d30>])),\n",
       " ('model_217',\n",
       "  (0.27556002140045166, [<ruben.PersistenceDiagram at 0x13b110be0>])),\n",
       " ('model_216',\n",
       "  (0.2773599624633789, [<ruben.PersistenceDiagram at 0x13b928df0>])),\n",
       " ('model_148',\n",
       "  (0.262440025806427, [<ruben.PersistenceDiagram at 0x13c012730>])),\n",
       " ('model_606',\n",
       "  (0.1928200125694275, [<ruben.PersistenceDiagram at 0x13be68f70>])),\n",
       " ('model_661',\n",
       "  (0.18076002597808838, [<ruben.PersistenceDiagram at 0x13bc1b220>])),\n",
       " ('model_729',\n",
       "  (0.21345996856689453, [<ruben.PersistenceDiagram at 0x13c137ee0>])),\n",
       " ('model_728',\n",
       "  (0.21277999877929688, [<ruben.PersistenceDiagram at 0x13c5f7820>])),\n",
       " ('model_660',\n",
       "  (0.18088001012802124, [<ruben.PersistenceDiagram at 0x13bbd5520>])),\n",
       " ('model_662',\n",
       "  (0.21999996900558472, [<ruben.PersistenceDiagram at 0x13c86d5e0>])),\n",
       " ('model_89',\n",
       "  (0.28370004892349243, [<ruben.PersistenceDiagram at 0x13c81b160>])),\n",
       " ('model_88',\n",
       "  (0.29089999198913574, [<ruben.PersistenceDiagram at 0x13d277340>])),\n",
       " ('model_663',\n",
       "  (0.20893996953964233, [<ruben.PersistenceDiagram at 0x13d0a7940>])),\n",
       " ('model_667',\n",
       "  (0.3070399761199951, [<ruben.PersistenceDiagram at 0x13d44ac40>])),\n",
       " ('model_539',\n",
       "  (0.2968599796295166, [<ruben.PersistenceDiagram at 0x13e208e20>])),\n",
       " ('model_538',\n",
       "  (0.28797996044158936, [<ruben.PersistenceDiagram at 0x13dfc5bb0>])),\n",
       " ('model_666',\n",
       "  (0.30952000617980957, [<ruben.PersistenceDiagram at 0x13d2fb850>])),\n",
       " ('model_664',\n",
       "  (0.20735996961593628, [<ruben.PersistenceDiagram at 0x13e234940>])),\n",
       " ('model_670',\n",
       "  (0.1629999876022339, [<ruben.PersistenceDiagram at 0x13db8ba00>])),\n",
       " ('model_671',\n",
       "  (0.15939998626708984, [<ruben.PersistenceDiagram at 0x13e45fac0>])),\n",
       " ('model_665',\n",
       "  (0.22387999296188354, [<ruben.PersistenceDiagram at 0x13e97ac40>])),\n",
       " ('model_668',\n",
       "  (0.13599997758865356, [<ruben.PersistenceDiagram at 0x13e9b5e20>])),\n",
       " ('model_536',\n",
       "  (0.2294600009918213, [<ruben.PersistenceDiagram at 0x13edd0400>])),\n",
       " ('model_734',\n",
       "  (0.18569999933242798, [<ruben.PersistenceDiagram at 0x13f171c40>])),\n",
       " ('model_735',\n",
       "  (0.18206000328063965, [<ruben.PersistenceDiagram at 0x13f0884c0>])),\n",
       " ('model_537',\n",
       "  (0.24528002738952637, [<ruben.PersistenceDiagram at 0x13f088f10>])),\n",
       " ('model_669',\n",
       "  (0.1400800347328186, [<ruben.PersistenceDiagram at 0x13f338580>])),\n",
       " ('model_94',\n",
       "  (0.2912200093269348, [<ruben.PersistenceDiagram at 0x13f4df460>])),\n",
       " ('model_535',\n",
       "  (0.21053999662399292, [<ruben.PersistenceDiagram at 0x13f7c8130>])),\n",
       " ('model_534',\n",
       "  (0.21494001150131226, [<ruben.PersistenceDiagram at 0x13fa70af0>])),\n",
       " ('model_95',\n",
       "  (0.2755800485610962, [<ruben.PersistenceDiagram at 0x13f90cd90>])),\n",
       " ('model_85',\n",
       "  (0.25787997245788574, [<ruben.PersistenceDiagram at 0x13fba65b0>])),\n",
       " ('model_91',\n",
       "  (0.31025999784469604, [<ruben.PersistenceDiagram at 0x140209a30>])),\n",
       " ('model_726',\n",
       "  (0.21735996007919312, [<ruben.PersistenceDiagram at 0x13ff01b20>])),\n",
       " ('model_732',\n",
       "  (0.17607998847961426, [<ruben.PersistenceDiagram at 0x13fda8e20>])),\n",
       " ('model_733',\n",
       "  (0.15526002645492554, [<ruben.PersistenceDiagram at 0x140137eb0>])),\n",
       " ('model_727',\n",
       "  (0.20862001180648804, [<ruben.PersistenceDiagram at 0x1404133a0>])),\n",
       " ('model_90',\n",
       "  (0.3316799998283386, [<ruben.PersistenceDiagram at 0x1406f1ca0>])),\n",
       " ('model_84',\n",
       "  (0.2646600008010864, [<ruben.PersistenceDiagram at 0x140deb4c0>])),\n",
       " ('model_92',\n",
       "  (0.26569998264312744, [<ruben.PersistenceDiagram at 0x140a21d60>])),\n",
       " ('model_86',\n",
       "  (0.2817800045013428, [<ruben.PersistenceDiagram at 0x140c9bfa0>])),\n",
       " ('model_533',\n",
       "  (0.18654000759124756, [<ruben.PersistenceDiagram at 0x1414622e0>])),\n",
       " ('model_731',\n",
       "  (0.30355995893478394, [<ruben.PersistenceDiagram at 0x14119d0a0>])),\n",
       " ('model_725',\n",
       "  (0.16460001468658447, [<ruben.PersistenceDiagram at 0x141242f70>])),\n",
       " ('model_724',\n",
       "  (0.1732199788093567, [<ruben.PersistenceDiagram at 0x14165e760>])),\n",
       " ('model_730',\n",
       "  (0.3127400279045105, [<ruben.PersistenceDiagram at 0x1420f79a0>])),\n",
       " ('model_532',\n",
       "  (0.18598002195358276, [<ruben.PersistenceDiagram at 0x141be3820>])),\n",
       " ('model_87',\n",
       "  (0.2853599786758423, [<ruben.PersistenceDiagram at 0x141c21130>])),\n",
       " ('model_93',\n",
       "  (0.2522599697113037, [<ruben.PersistenceDiagram at 0x14229d970>])),\n",
       " ('model_151',\n",
       "  (0.2824999690055847, [<ruben.PersistenceDiagram at 0x142c2e790>])),\n",
       " ('model_596',\n",
       "  (0.1851000189781189, [<ruben.PersistenceDiagram at 0x1426fc610>])),\n",
       " ('model_23',\n",
       "  (0.28035998344421387, [<ruben.PersistenceDiagram at 0x1427367c0>])),\n",
       " ('model_541',\n",
       "  (0.14727997779846191, [<ruben.PersistenceDiagram at 0x14303b550>])),\n",
       " ('model_540',\n",
       "  (0.1441599726676941, [<ruben.PersistenceDiagram at 0x143b1f730>])),\n",
       " ('model_22',\n",
       "  (0.2800599932670593, [<ruben.PersistenceDiagram at 0x143eb0460>])),\n",
       " ('model_597',\n",
       "  (0.18365997076034546, [<ruben.PersistenceDiagram at 0x143782eb0>])),\n",
       " ('model_150',\n",
       "  (0.28633999824523926, [<ruben.PersistenceDiagram at 0x143e97c70>])),\n",
       " ('model_152',\n",
       "  (0.2825400233268738, [<ruben.PersistenceDiagram at 0x143e6d220>])),\n",
       " ('model_20',\n",
       "  (0.2666199803352356, [<ruben.PersistenceDiagram at 0x14387fbe0>])),\n",
       " ('model_218',\n",
       "  (0.3247600197792053, [<ruben.PersistenceDiagram at 0x14407eca0>])),\n",
       " ('model_542',\n",
       "  (0.1672000288963318, [<ruben.PersistenceDiagram at 0x144227c40>])),\n",
       " ('model_543',\n",
       "  (0.16740000247955322, [<ruben.PersistenceDiagram at 0x143f69c40>])),\n",
       " ('model_219',\n",
       "  (0.31616002321243286, [<ruben.PersistenceDiagram at 0x14450dd00>])),\n",
       " ('model_21',\n",
       "  (0.2628600001335144, [<ruben.PersistenceDiagram at 0x14477cdc0>])),\n",
       " ('model_153',\n",
       "  (0.27903997898101807, [<ruben.PersistenceDiagram at 0x1449820a0>])),\n",
       " ('model_25',\n",
       "  (0.2765600085258484, [<ruben.PersistenceDiagram at 0x144ef6220>])),\n",
       " ('model_31',\n",
       "  (0.257099986076355, [<ruben.PersistenceDiagram at 0x144a89040>])),\n",
       " ('model_30',\n",
       "  (0.2638000249862671, [<ruben.PersistenceDiagram at 0x14536ad60>])),\n",
       " ('model_24',\n",
       "  (0.28283995389938354, [<ruben.PersistenceDiagram at 0x1454cbdf0>])),\n",
       " ('model_154',\n",
       "  (0.31845998764038086, [<ruben.PersistenceDiagram at 0x1456b1610>])),\n",
       " ('model_222',\n",
       "  (0.28082001209259033, [<ruben.PersistenceDiagram at 0x145b13c70>])),\n",
       " ('model_26',\n",
       "  (0.3172000050544739, [<ruben.PersistenceDiagram at 0x145f3beb0>])),\n",
       " ('model_27',\n",
       "  (0.3140000104904175, [<ruben.PersistenceDiagram at 0x145d22f70>])),\n",
       " ('model_223',\n",
       "  (0.2788999676704407, [<ruben.PersistenceDiagram at 0x145d22fa0>])),\n",
       " ('model_155',\n",
       "  (0.3206999897956848, [<ruben.PersistenceDiagram at 0x1461394f0>]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pds_and_generalization_gaps(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TASK_1_pds_and_generalization_gaps():\n",
    "    return get_pds_and_generalization_gaps(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TASK_2_pds_and_generalization_gaps():\n",
    "    return get_pds_and_generalization_gaps(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_test(gen_x_pds, train_size=0.7):\n",
    "    training_elems = int(math.floor(train_size*len(gen_x_pds)))\n",
    "    return gen_x_pds[:training_elems], gen_x_pds[training_elems:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_train_and_test(gen_x_pds):\n",
    "    shuffled_gen_x_pds = shuffle(gen_x_pds)\n",
    "    return split_train_and_test(shuffled_gen_x_pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_x_gen_task1 = get_TASK_1_pds_and_generalization_gaps()\n",
    "# pds_x_gen_task2 = get_TASK_2_pds_and_generalization_gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_linear_model():\n",
    "    return LinearRegression()\n",
    "def ridge_linear_model():\n",
    "    return linear_model.Ridge(alpha=.5)\n",
    "def lasso_linear_model():\n",
    "    return linear_model.Lasso(alpha=0.1)\n",
    "def elasticNet():\n",
    "    return ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for extracting features\n",
    "\n",
    "See the other notebooks to see detailed descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruben import PersistenceDiagram\n",
    "# from model.TopologicalSummaries.PersistenceDiagramSummaries import average_life, average_midlife\n",
    "class AllDims:\n",
    "    def __contains__(self, _): return True\n",
    "\n",
    "\n",
    "all_dims = AllDims() # cannot be static method\n",
    "\n",
    "def is_collection(x):\n",
    "    return hasattr(x,\"__contains__\")\n",
    "\n",
    "def alpha_weighted_average_life(persistence_diagram: PersistenceDiagram, alpha: float, dimensions=all_dims):\n",
    "    if not is_collection(dimensions): dimensions = [dimensions]\n",
    "    points = filter(lambda point: point.dim in dimensions, persistence_diagram.points)\n",
    "\n",
    "    life = lambda point: point.death - point.birth\n",
    "    values = map(lambda point: life(point)*life(point)**alpha, points)\n",
    "\n",
    "    val_without_inf = filter(lambda val: not np.isinf(val), values)\n",
    "    return np.mean(np.fromiter(val_without_inf, dtype=np.double))\n",
    "\n",
    "def alpha_weighted_average_midlife(persistence_diagram: PersistenceDiagram, alpha: float, dimensions=all_dims):\n",
    "    if not is_collection(dimensions): dimensions = [dimensions]\n",
    "    points = filter(lambda point: point.dim in dimensions, persistence_diagram.points)\n",
    "\n",
    "    life = lambda point: point.death - point.birth\n",
    "    midlife = lambda point: (point.death + point.birth)/2\n",
    "    values = map(lambda point: midlife(point)*life(point)**alpha, points)\n",
    "\n",
    "    val_without_inf = filter(lambda val: not np.isinf(val), values)\n",
    "    return np.mean(np.fromiter(val_without_inf, dtype=np.double))\n",
    "\n",
    "def average_life(persistence_diagram: PersistenceDiagram, dimensions=all_dims):\n",
    "    return alpha_weighted_average_life(persistence_diagram, 0, dimensions=dimensions)\n",
    "\n",
    "def average_midlife(persistence_diagram: PersistenceDiagram, dimensions=all_dims):\n",
    "    return alpha_weighted_average_midlife(persistence_diagram, 0, dimensions=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooling_vector(persistence_diagram, dims, num_elems=10):\n",
    "        finite_points_pd = persistence_diagram.get_finite_points()\n",
    "        pooling_vector = []\n",
    "        for dim in dims:\n",
    "            points_of_dim = filter(lambda point: point.dim == dim, finite_points_pd)\n",
    "            persistences = list(map(lambda point: point.death - point.birth, points_of_dim))\n",
    "            persistences.sort(reverse=True)\n",
    "            pooling_vector.extend(persistences[:num_elems])\n",
    "            #If we don't have enough persistences we fill the vector with 0s\n",
    "            if(len(persistences) < num_elems):\n",
    "                pooling_vector.extend([0]*(num_elems - len(persistences)))\n",
    "        return np.array(pooling_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_life_midlife_vector(persistence_diagram, dims):\n",
    "    lifes_midlifes = []\n",
    "    for dim in dims:\n",
    "        lifes_midlifes.append(average_life(persistence_diagram, dimensions=[dim]))\n",
    "        lifes_midlifes.append(average_midlife(persistence_diagram, dimensions=[dim]))\n",
    "    lifes_midlifes.append(average_life(persistence_diagram))\n",
    "    lifes_midlifes.append(average_midlife(persistence_diagram))\n",
    "    return np.nan_to_num(np.array(lifes_midlifes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_life_midlife_vector_squared(persistence_diagram, dims):\n",
    "    lifes_midlifes = []\n",
    "    for dim in dims:\n",
    "        lifes_midlifes.append(average_life(persistence_diagram, dimensions=[dim]))\n",
    "        lifes_midlifes.append(average_midlife(persistence_diagram, dimensions=[dim]))\n",
    "    lifes_midlifes.append(average_life(persistence_diagram))\n",
    "    lifes_midlifes.append(average_midlife(persistence_diagram))\n",
    "    life_midlife_array = np.nan_to_num(np.array(lifes_midlifes))\n",
    "    return np.hstack([life_midlife_array, life_midlife_array**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_birth_death(points):\n",
    "    birth_death_pairs = np.array(list(map(lambda point: [point.birth, point.death], points)))\n",
    "    if birth_death_pairs.shape[0] == 0:\n",
    "        return [0, 0]\n",
    "    return np.average(birth_death_pairs, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    return np.array(birth_eath_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_squared(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_death_vector = np.array(birth_eath_averages)\n",
    "    return np.hstack([birth_death_vector, birth_death_vector**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_inverted(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_death = np.array(birth_eath_averages)\n",
    "    return np.hstack([np.log(birth_death+1), birth_death, np.nan_to_num(np.reciprocal(birth_death, out=np.zeros_like(birth_death), where=birth_death!=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_life_midlife_vector(persistence_diagram, dims):\n",
    "    birth_death = get_average_birth_death_vector(persistence_diagram, dims)\n",
    "    life_midlife = get_life_midlife_vector(persistence_diagram, dims)\n",
    "    return np.hstack([birth_death**2, life_midlife**2, birth_death, life_midlife])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(persistence_diagram, dims):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    pe = PersistenceEntropy()\n",
    "    pe_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            pe_vector.append(0)\n",
    "        else:\n",
    "            pe_vector.append(pe.fit_transform(points_3d)[0][0])\n",
    "    return np.array(pe_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_points(persistence_diagram, dims):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    numPoints = NumberOfPoints()\n",
    "    numPoints_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            numPoints_vector.append(0)\n",
    "        else:\n",
    "            numPoints_vector.append(numPoints.fit_transform(points_3d)[0][0])\n",
    "    return np.array(numPoints_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persistence_image_vector(persistence_diagram, dims, n_bins=100):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    pi = PersistenceImage(sigma=0.1, n_bins=n_bins)\n",
    "    pi_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            pi_vector.append(np.zeros(shape=(n_bins,n_bins)))\n",
    "        else:\n",
    "            pi_vector.append(pi.fit_transform(points_3d)[0][0])\n",
    "    return np.array(pi_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_and_std_birth_and_death(points):\n",
    "    if len(points) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    births = np.array(list(map(lambda point: point.birth, points)))\n",
    "    deaths = np.array(list(map(lambda point: point.death, points)))\n",
    "    return [np.std(births), np.std(deaths), np.mean(births), np.mean(deaths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_and_std_birth_and_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    return np.array(birth_eath_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_squared(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_and_std_birth_and_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages_squared = np.array(birth_eath_averages)\n",
    "    return np.hstack([birth_eath_averages_squared, birth_eath_averages_squared**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_roots(persistence_diagram, dims, n_coefficients=10, pol_type='T'):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    complex_pol = ComplexPolynomial(n_coefficients=n_coefficients, polynomial_type=pol_type)\n",
    "    complex_pol_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            complex_pol_vector.extend([0]*(2*n_coefficients))\n",
    "        else:\n",
    "            complex_pol_vector.extend(complex_pol.fit_transform(points_3d)[0])\n",
    "    return np.array(complex_pol_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un dict con todas las features para todos los modelos de task 1 y task 2 y guardarlo en memoria para manipularlo, luego hacer estudios, que ya veremos cuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def bootstraping_function(X, pd_to_vec_fn, dims, samples_per_try=20, tries=5): #Change to 20, 5\n",
    "    def bootstraping_elem(X_elem):\n",
    "        vec_values_per_try = []\n",
    "        for try_it in range(tries):\n",
    "            try_sample = []\n",
    "            for sample_it in range(samples_per_try):\n",
    "                choice = random.randint(0, len(X_elem) - 1)\n",
    "                try_sample.append(pd_to_vec_fn(X_elem[choice], dims))\n",
    "            vec_values_per_try.append(np.mean(np.array(try_sample), axis=0))\n",
    "        return np.mean(np.vstack(vec_values_per_try), axis=0)\n",
    "    return list(map(bootstraping_elem, tqdm(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstraped_features(pds_x_gens, pd_to_vec_fn):\n",
    "    working_dims=[('dim0', [0]), ('dim1', [1]), ('dim01',[0,1])]\n",
    "    def get_pds_and_gen(pds_gen):\n",
    "        _, gen_x_pds = list(zip(*pds_gen))\n",
    "        return list(zip(*gen_x_pds))\n",
    "    features_x_dims = dict()\n",
    "    y, X_raw = get_pds_and_gen(pds_x_gens)\n",
    "    for hashstr, dims in working_dims:\n",
    "        # X_feature = bootstraping_function(X_raw, pd_to_vec_fn, dims)\n",
    "        X_feature = list(map(lambda pd: pd_to_vec_fn(pd[0], dims), tqdm(X_raw)))\n",
    "        features_x_dims[hashstr] = X_feature\n",
    "    return y, features_x_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Per each feature and task we return a dictionary with the features computed at dims == key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 31.00it/s]\n",
      "100%|██████████| 90/90 [00:02<00:00, 30.02it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 27.79it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, pooling_task1 = get_bootstraped_features(pds_x_gen_task1, get_pooling_vector)\n",
    "# y_values, pooling_task2 = get_bootstraped_features(pds_x_gen_task2, get_pooling_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:19<00:00,  4.71it/s]\n",
      "100%|██████████| 90/90 [00:25<00:00,  3.51it/s]\n",
      "100%|██████████| 90/90 [00:31<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, life_midlife_task1 = get_bootstraped_features(pds_x_gen_task1, get_life_midlife_vector)\n",
    "#y_values, life_midlife_task2 = get_bootstraped_features(pds_x_gen_task2, get_life_midlife_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:21<00:00,  4.21it/s]\n",
      "100%|██████████| 90/90 [00:27<00:00,  3.29it/s]\n",
      "100%|██████████| 90/90 [00:32<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, life_midlife_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_life_midlife_vector_squared)\n",
    "#y_values, life_midlife_squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_life_midlife_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 31.57it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 23.22it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 26.75it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector)\n",
    "#y_values, average_birth_death_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 33.72it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 23.46it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 21.19it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_squared)\n",
    "#y_values, average_birth_death_squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 30.28it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 21.08it/s]\n",
      "100%|██████████| 90/90 [00:05<00:00, 15.89it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_inverted_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_inverted)\n",
    "#y_values, average_birth_death_inverted_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:18<00:00,  4.79it/s]\n",
      "100%|██████████| 90/90 [00:30<00:00,  2.99it/s]\n",
      "100%|██████████| 90/90 [00:32<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_life_midlife_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_life_midlife_vector)\n",
    "#y_values, average_birth_death_life_midlife_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_life_midlife_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:03<00:00, 29.69it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 22.76it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 18.35it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, entropy_task1 = get_bootstraped_features(pds_x_gen_task1, get_entropy)\n",
    "#y_values, entropy_task2 = get_bootstraped_features(pds_x_gen_task2, get_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:03<00:00, 29.36it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 20.85it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 18.32it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, number_of_points_task1 = get_bootstraped_features(pds_x_gen_task1, get_number_of_points)\n",
    "#y_values, number_of_points_task2 = get_bootstraped_features(pds_x_gen_task2, get_number_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:03<00:00, 29.43it/s]\n",
      "100%|██████████| 90/90 [00:03<00:00, 24.97it/s]\n",
      "100%|██████████| 90/90 [00:04<00:00, 19.00it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_std_birth_death_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector)\n",
    "#y_values, average_std_birth_death_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:03<00:00, 23.49it/s]\n",
      "100%|██████████| 90/90 [00:06<00:00, 13.24it/s]\n",
      "100%|██████████| 90/90 [00:06<00:00, 14.98it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_std_birth_death_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_squared)\n",
    "#y_values, average_std_birth_death_Squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:19<00:00,  4.55it/s]\n",
      "100%|██████████| 90/90 [02:33<00:00,  1.71s/it]\n",
      "100%|██████████| 90/90 [03:19<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, complex_roots_task1 = get_bootstraped_features(pds_x_gen_task1, get_complex_roots)\n",
    "#y_values, complex_roots_task2 = get_bootstraped_features(pds_x_gen_task2, get_complex_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1 = dict()\n",
    "# all_features_task2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['pooling'] = pooling_task1\n",
    "# all_features_task2['pooling'] = pooling_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['life_midlife'] = life_midlife_task1\n",
    "# all_features_task2['life_midlife'] = life_midlife_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['life_midlife_squared'] = life_midlife_squared_task1\n",
    "#all_features_task2['life_midlife_squared'] = life_midlife_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death'] = average_birth_death_task1\n",
    "#all_features_task2['average_birth_death'] = average_birth_death_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_squared'] = average_birth_death_squared_task1\n",
    "#all_features_task2['average_birth_death_squared'] = average_birth_death_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_inverted'] = average_birth_death_inverted_task1\n",
    "#all_features_task2['average_birth_death_inverted'] = average_birth_death_inverted_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_life_midlife'] = average_birth_death_life_midlife_task1\n",
    "#all_features_task2['average_birth_death_life_midlife'] = average_birth_death_life_midlife_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['entropy'] = entropy_task1\n",
    "#all_features_task2['entropy'] = entropy_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['number_of_points'] = number_of_points_task1\n",
    "#all_features_task2['number_of_points'] = number_of_points_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_std_birth_death'] = average_std_birth_death_task1\n",
    "#all_features_task2['average_std_birth_death'] = average_std_birth_death_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_std_birth_death_squared'] = average_std_birth_death_squared_task1\n",
    "#all_features_task2['average_std_birth_death_squared'] = average_std_birth_death_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['complex_roots'] = complex_roots_task1\n",
    "#all_features_task2['complex_roots'] = complex_roots_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(outdir / '../../all_features_task1importancef2000.pickle', 'wb') as handle1:\n",
    "    pickle.dump(all_features_task1, handle1, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('all_features_task2.pickle', 'wb') as handle2:\n",
    "#     pickle.dump(all_features_task2, handle2, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start working on mixed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir / '../all_features_task1importancef2000.pickle', 'rb') as handle1:\n",
    "    all_features_task1 = pickle.load(handle1)\n",
    "\n",
    "# with open('all_features_task2.pickle', 'rb') as handle2:\n",
    "#     all_features_task2 = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, it starts the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_linear_model():\n",
    "    return LinearRegression()\n",
    "def ridge_linear_model():\n",
    "    return linear_model.Ridge(alpha=.5)\n",
    "def lasso_linear_model():\n",
    "    return linear_model.Lasso(alpha=0.1)\n",
    "def elasticNet():\n",
    "    return ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate y train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_values(pds_x_gens):\n",
    "    def get_pds_and_gen(pds_gen):\n",
    "        _, gen_x_pds = list(zip(*pds_gen))\n",
    "        return list(zip(*gen_x_pds))\n",
    "    features_x_dims = dict()\n",
    "    y, _ = get_pds_and_gen(pds_x_gens)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_task1 = get_y_values(pds_x_gen_task1)\n",
    "# y_task2 = get_y_values(pds_x_gen_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_combinations_without_empty_set(all_features):\n",
    "    possible_combinations = list()\n",
    "    keys = all_features.keys()\n",
    "    for cardinality in range(1, len(keys) + 1):\n",
    "        possible_combinations.extend(combinations(keys, cardinality))\n",
    "    return possible_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = get_all_possible_combinations_without_empty_set(all_features_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(all_features, y, possible_dimensions=['dim0', 'dim1', 'dim01'], test_size=0.3, create_model_fn=classic_linear_model, number_of_experiments=10):\n",
    "    results = dict()\n",
    "    possible_combinations = get_all_possible_combinations_without_empty_set(all_features)\n",
    "    for possible_combination in tqdm(possible_combinations):\n",
    "        features_to_train = list()\n",
    "        for feature in possible_combination:\n",
    "            for possible_dimension in possible_dimensions:\n",
    "                features_to_train.append(all_features[feature][possible_dimension])\n",
    "        X = np.hstack(features_to_train)\n",
    "        scores = list()\n",
    "        for _ in range(number_of_experiments):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "            reg = create_model_fn().fit(X_train, y_train)\n",
    "            scores.append(reg.score(X_test, y_test))\n",
    "        results[tuple(possible_combination)] = np.mean(scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4095/4095 [03:30<00:00, 19.41it/s]\n"
     ]
    }
   ],
   "source": [
    "results_task1 = generate_experiment(all_features_task1, y_task1, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_task2 = generate_experiment(all_features_task2, y_task2, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('average_birth_death',\n",
       "  'average_birth_death_squared',\n",
       "  'average_birth_death_life_midlife',\n",
       "  'average_std_birth_death'),\n",
       " 0.8935729120890972)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task1.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('life_midlife_squared',\n",
       "  'average_birth_death',\n",
       "  'average_birth_death_squared',\n",
       "  'entropy',\n",
       "  'average_std_birth_death_squared'),\n",
       " 0.9009350185767977)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task2.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_combinations_without_empty_set_all(selected_features_keys, possible_dimensions=['dim0', 'dim1', 'dim01']):\n",
    "    possible_combinations = list()\n",
    "    keys = list()\n",
    "    for key_raw in selected_features_keys:\n",
    "        for possible_dim in possible_dimensions:\n",
    "            keys.append((key_raw, possible_dim))\n",
    "    for cardinality in range(1, len(keys) + 1):\n",
    "        possible_combinations.extend(combinations(keys, cardinality))\n",
    "    return possible_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_all(all_features, selected_features_keys, y, possible_dimensions=['dim0', 'dim1', 'dim01'], test_size=0.3, create_model_fn=classic_linear_model, number_of_experiments=10):\n",
    "    results = dict()\n",
    "    possible_combinations = get_all_possible_combinations_without_empty_set_all(selected_features_keys, possible_dimensions=possible_dimensions)\n",
    "    for possible_combination in possible_combinations:\n",
    "        features_to_train = list()\n",
    "        for feature, dim in possible_combination:\n",
    "            features_to_train.append(all_features[feature][dim])\n",
    "        X = np.hstack(features_to_train)\n",
    "        scores = list()\n",
    "        try:\n",
    "            for _ in range(number_of_experiments):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "                reg = create_model_fn().fit(X_train, y_train)\n",
    "                scores.append(reg.score(X_test, y_test))\n",
    "            results[tuple(possible_combination)] = np.mean(scores)\n",
    "            \n",
    "        except:\n",
    "            print('The next combination cannot be trained with a linear model')\n",
    "            print(possible_combination)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task1 = generate_experiment_all(all_features_task1, (\n",
    "    'life_midlife',\n",
    "  'life_midlife_squared',\n",
    "    'average_birth_death',\n",
    "  'entropy',\n",
    "    'average_std_birth_death',\n",
    "  'average_std_birth_death_squared'), y_task1, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task2 = generate_experiment_all(all_features_task2, (\n",
    "    'life_midlife',\n",
    "  'life_midlife_squared',\n",
    "    'average_birth_death',\n",
    "  'entropy',\n",
    "    'average_std_birth_death',\n",
    "  'average_std_birth_death_squared'), y_task2, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('life_midlife_squared', 'dim0'),\n",
       "  ('average_std_birth_death', 'dim01'),\n",
       "  ('average_std_birth_death_squared', 'dim0')),\n",
       " 0.9243755781111016)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task1.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('entropy', 'dim0'),\n",
       "  ('entropy', 'dim1'),\n",
       "  ('average_std_birth_death', 'dim01'),\n",
       "  ('average_std_birth_death_squared', 'dim0')),\n",
       " 0.9230907791163198)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task2.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 29 2022, 19:40:25) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "81226aa14f23f9900317dc9fcf2c01924595c28b8e9ad492381f4125680523ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
