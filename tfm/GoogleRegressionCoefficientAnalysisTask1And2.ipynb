{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding NeurIPS to the path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruben import PersistenceDiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "import math\n",
    "from gtda.diagrams import PersistenceEntropy, NumberOfPoints, Amplitude, PersistenceImage, ComplexPolynomial, BettiCurve\n",
    "from functools import partial \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import random\n",
    "# from model.Google.MutualInformationGoogle import get_mutual_information\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "def get_gen_gaps_folder(task_number):\n",
    "    return os.path.abspath('../../NeurIPSSoftware/results/task{}/generalization_gap'.format(task_number))\n",
    "def get_pds_folder(task_number):\n",
    "    return os.path.abspath('../../out/task{}/pds'.format(task_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from main import DATA_PATH\n",
    "\n",
    "def load_generalization_gap(model_file, folder_generalization):\n",
    "    with open('{}/{}'.format(folder_generalization, model_file), 'rb') as file:\n",
    "        gen_gap = pickle.load(file)\n",
    "    return gen_gap\n",
    "\n",
    "def load_generalization_gaps(task_number):\n",
    "    # folder_path = get_gen_gaps_folder(task_number)\n",
    "    # model_names_w_ext = os.listdir(folder_path)\n",
    "    # model_names = map(lambda name: name[:-4], model_names_w_ext)\n",
    "    # gen_gaps = map(lambda model_name: load_generalization_gap(model_name, folder_path), model_names_w_ext)\n",
    "    # return dict(zip(model_names, gen_gaps))\n",
    "    with open(DATA_PATH / '../../reference_data/task1_v4/model_configs.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    return {'model_' + k: v['metrics']['train_acc'] - v['metrics']['test_acc'] for k, v in config.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outdir = Path('../out/task1/pds/importance')\n",
    "\n",
    "def load_pd(path):\n",
    "    # with open('{}/{}'.format(folder_pd, model_file), 'rb') as file:\n",
    "    #     pd = pickle.loads(file.read().replace(b'model.PersistenceDiagram',b'model.homology.PersistenceDiagram'))\n",
    "    # return pd\n",
    "    with open(path, 'rb') as f:\n",
    "        # return pickle.loads(f.read().replace(b'model.PersistenceDiagram',b'model.homology.PersistenceDiagram'))\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_pds(task_number):\n",
    "    # folder_path = get_pds_folder(task_number)\n",
    "    # pds_tries = os.listdir(folder_path)\n",
    "    # pds_by_model_number = dict()\n",
    "    # for pd_try in pds_tries:\n",
    "    #     try_path = '{}/{}'.format(folder_path, pd_try)\n",
    "    #     model_names_w_ext = os.listdir(try_path)\n",
    "    #     for model_name_w_ext in model_names_w_ext:\n",
    "    #         model_name = model_name_w_ext[:-4]\n",
    "    #         pd = load_pd(model_name_w_ext, try_path)\n",
    "    #         if model_name not in pds_by_model_number:\n",
    "    #             pds_by_model_number[model_name] = [pd]\n",
    "    #         else:\n",
    "    #             pds_by_model_number[model_name].append(pd)\n",
    "    # return pds_by_model_number\n",
    "    assert task_number == 1\n",
    "    return {p.stem: [load_pd(p)] for p in outdir.glob('model_*.bin')}\n",
    "\n",
    "def get_pds_and_generalization_gaps(task_number):\n",
    "    pds = load_pds(task_number)\n",
    "    gen_gaps = load_generalization_gaps(task_number)\n",
    "    model_names = pds.keys()\n",
    "    gen_gaps_and_pds = dict()\n",
    "    for model_name in model_names:\n",
    "        gen_gaps_and_pds[model_name] = (gen_gaps[model_name], pds[model_name])\n",
    "    return list(gen_gaps_and_pds.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model_158',\n",
       "  (0.2593799829483032, [<ruben.PersistenceDiagram at 0x173e3c310>])),\n",
       " ('model_602',\n",
       "  (0.30239999294281006, [<ruben.PersistenceDiagram at 0x175478af0>])),\n",
       " ('model_212',\n",
       "  (0.25919997692108154, [<ruben.PersistenceDiagram at 0x142f85c90>])),\n",
       " ('model_213',\n",
       "  (0.2612000107765198, [<ruben.PersistenceDiagram at 0x17547c730>])),\n",
       " ('model_603',\n",
       "  (0.3065999746322632, [<ruben.PersistenceDiagram at 0x17561d240>])),\n",
       " ('model_159',\n",
       "  (0.2656399607658386, [<ruben.PersistenceDiagram at 0x1756959c0>])),\n",
       " ('model_601',\n",
       "  (0.23843997716903687, [<ruben.PersistenceDiagram at 0x1756ab640>])),\n",
       " ('model_29',\n",
       "  (0.24706000089645386, [<ruben.PersistenceDiagram at 0x1756ab670>])),\n",
       " ('model_28',\n",
       "  (0.2514600157737732, [<ruben.PersistenceDiagram at 0x175683bb0>])),\n",
       " ('model_600',\n",
       "  (0.23054003715515137, [<ruben.PersistenceDiagram at 0x1753b9180>])),\n",
       " ('model_604',\n",
       "  (0.17900002002716064, [<ruben.PersistenceDiagram at 0x17589b4f0>])),\n",
       " ('model_599',\n",
       "  (0.20836001634597778, [<ruben.PersistenceDiagram at 0x1759ea650>])),\n",
       " ('model_214',\n",
       "  (0.2841799855232239, [<ruben.PersistenceDiagram at 0x175bcb850>])),\n",
       " ('model_215',\n",
       "  (0.28255999088287354, [<ruben.PersistenceDiagram at 0x1759ebfa0>])),\n",
       " ('model_598',\n",
       "  (0.2165200114250183, [<ruben.PersistenceDiagram at 0x175ab0be0>])),\n",
       " ('model_605',\n",
       "  (0.1578199863433838, [<ruben.PersistenceDiagram at 0x175e6b910>])),\n",
       " ('model_607',\n",
       "  (0.17698001861572266, [<ruben.PersistenceDiagram at 0x17612c550>])),\n",
       " ('model_149',\n",
       "  (0.25936001539230347, [<ruben.PersistenceDiagram at 0x175d91ab0>])),\n",
       " ('model_217',\n",
       "  (0.27556002140045166, [<ruben.PersistenceDiagram at 0x176021030>])),\n",
       " ('model_216',\n",
       "  (0.2773599624633789, [<ruben.PersistenceDiagram at 0x176269630>])),\n",
       " ('model_148',\n",
       "  (0.262440025806427, [<ruben.PersistenceDiagram at 0x1762e75b0>])),\n",
       " ('model_606',\n",
       "  (0.1928200125694275, [<ruben.PersistenceDiagram at 0x1763e3bb0>])),\n",
       " ('model_661',\n",
       "  (0.18076002597808838, [<ruben.PersistenceDiagram at 0x1763f6170>])),\n",
       " ('model_729',\n",
       "  (0.21345996856689453, [<ruben.PersistenceDiagram at 0x1764c7970>])),\n",
       " ('model_728',\n",
       "  (0.21277999877929688, [<ruben.PersistenceDiagram at 0x1765c7f70>])),\n",
       " ('model_660',\n",
       "  (0.18088001012802124, [<ruben.PersistenceDiagram at 0x176793f70>])),\n",
       " ('model_662',\n",
       "  (0.21999996900558472, [<ruben.PersistenceDiagram at 0x1768d1c30>])),\n",
       " ('model_89',\n",
       "  (0.28370004892349243, [<ruben.PersistenceDiagram at 0x1766ffe80>])),\n",
       " ('model_88',\n",
       "  (0.29089999198913574, [<ruben.PersistenceDiagram at 0x176968fd0>])),\n",
       " ('model_663',\n",
       "  (0.20893996953964233, [<ruben.PersistenceDiagram at 0x176a5e230>])),\n",
       " ('model_667',\n",
       "  (0.3070399761199951, [<ruben.PersistenceDiagram at 0x176b0a560>])),\n",
       " ('model_539',\n",
       "  (0.2968599796295166, [<ruben.PersistenceDiagram at 0x176ba8430>])),\n",
       " ('model_538',\n",
       "  (0.28797996044158936, [<ruben.PersistenceDiagram at 0x176cb8160>])),\n",
       " ('model_666',\n",
       "  (0.30952000617980957, [<ruben.PersistenceDiagram at 0x176dc6b00>])),\n",
       " ('model_664',\n",
       "  (0.20735996961593628, [<ruben.PersistenceDiagram at 0x176d60670>])),\n",
       " ('model_670',\n",
       "  (0.1629999876022339, [<ruben.PersistenceDiagram at 0x176feca00>])),\n",
       " ('model_671',\n",
       "  (0.15939998626708984, [<ruben.PersistenceDiagram at 0x176e50400>])),\n",
       " ('model_665',\n",
       "  (0.22387999296188354, [<ruben.PersistenceDiagram at 0x176ef94b0>])),\n",
       " ('model_668',\n",
       "  (0.13599997758865356, [<ruben.PersistenceDiagram at 0x177123a00>])),\n",
       " ('model_536',\n",
       "  (0.2294600009918213, [<ruben.PersistenceDiagram at 0x177277c40>])),\n",
       " ('model_734',\n",
       "  (0.18569999933242798, [<ruben.PersistenceDiagram at 0x17739fb20>])),\n",
       " ('model_735',\n",
       "  (0.18206000328063965, [<ruben.PersistenceDiagram at 0x177516140>])),\n",
       " ('model_537',\n",
       "  (0.24528002738952637, [<ruben.PersistenceDiagram at 0x1775a21a0>])),\n",
       " ('model_669',\n",
       "  (0.1400800347328186, [<ruben.PersistenceDiagram at 0x177678760>])),\n",
       " ('model_94',\n",
       "  (0.2912200093269348, [<ruben.PersistenceDiagram at 0x177810d60>])),\n",
       " ('model_535',\n",
       "  (0.21053999662399292, [<ruben.PersistenceDiagram at 0x177977700>])),\n",
       " ('model_534',\n",
       "  (0.21494001150131226, [<ruben.PersistenceDiagram at 0x177a6f340>])),\n",
       " ('model_95',\n",
       "  (0.2755800485610962, [<ruben.PersistenceDiagram at 0x177b85de0>])),\n",
       " ('model_85',\n",
       "  (0.25787997245788574, [<ruben.PersistenceDiagram at 0x177cbc820>])),\n",
       " ('model_91',\n",
       "  (0.31025999784469604, [<ruben.PersistenceDiagram at 0x177ddc700>])),\n",
       " ('model_726',\n",
       "  (0.21735996007919312, [<ruben.PersistenceDiagram at 0x177f67280>])),\n",
       " ('model_732',\n",
       "  (0.17607998847961426, [<ruben.PersistenceDiagram at 0x178027220>])),\n",
       " ('model_733',\n",
       "  (0.15526002645492554, [<ruben.PersistenceDiagram at 0x178125960>])),\n",
       " ('model_727',\n",
       "  (0.20862001180648804, [<ruben.PersistenceDiagram at 0x178338460>])),\n",
       " ('model_90',\n",
       "  (0.3316799998283386, [<ruben.PersistenceDiagram at 0x17836e260>])),\n",
       " ('model_84',\n",
       "  (0.2646600008010864, [<ruben.PersistenceDiagram at 0x1786051e0>])),\n",
       " ('model_92',\n",
       "  (0.26569998264312744, [<ruben.PersistenceDiagram at 0x17836e290>])),\n",
       " ('model_86',\n",
       "  (0.2817800045013428, [<ruben.PersistenceDiagram at 0x178558f10>])),\n",
       " ('model_533',\n",
       "  (0.18654000759124756, [<ruben.PersistenceDiagram at 0x17850b280>])),\n",
       " ('model_731',\n",
       "  (0.30355995893478394, [<ruben.PersistenceDiagram at 0x17876a320>])),\n",
       " ('model_725',\n",
       "  (0.16460001468658447, [<ruben.PersistenceDiagram at 0x1788ce920>])),\n",
       " ('model_724',\n",
       "  (0.1732199788093567, [<ruben.PersistenceDiagram at 0x178a39120>])),\n",
       " ('model_730',\n",
       "  (0.3127400279045105, [<ruben.PersistenceDiagram at 0x178a58460>])),\n",
       " ('model_532',\n",
       "  (0.18598002195358276, [<ruben.PersistenceDiagram at 0x178bb2bc0>])),\n",
       " ('model_87',\n",
       "  (0.2853599786758423, [<ruben.PersistenceDiagram at 0x178cbf1c0>])),\n",
       " ('model_93',\n",
       "  (0.2522599697113037, [<ruben.PersistenceDiagram at 0x178de5360>])),\n",
       " ('model_151',\n",
       "  (0.2824999690055847, [<ruben.PersistenceDiagram at 0x178e82440>])),\n",
       " ('model_596',\n",
       "  (0.1851000189781189, [<ruben.PersistenceDiagram at 0x178ee8e80>])),\n",
       " ('model_23',\n",
       "  (0.28035998344421387, [<ruben.PersistenceDiagram at 0x179031480>])),\n",
       " ('model_541',\n",
       "  (0.14727997779846191, [<ruben.PersistenceDiagram at 0x179146920>])),\n",
       " ('model_540',\n",
       "  (0.1441599726676941, [<ruben.PersistenceDiagram at 0x179189f60>])),\n",
       " ('model_22',\n",
       "  (0.2800599932670593, [<ruben.PersistenceDiagram at 0x17923bd60>])),\n",
       " ('model_597',\n",
       "  (0.18365997076034546, [<ruben.PersistenceDiagram at 0x17936ee60>])),\n",
       " ('model_150',\n",
       "  (0.28633999824523926, [<ruben.PersistenceDiagram at 0x179453460>])),\n",
       " ('model_152',\n",
       "  (0.2825400233268738, [<ruben.PersistenceDiagram at 0x1795979a0>])),\n",
       " ('model_20',\n",
       "  (0.2666199803352356, [<ruben.PersistenceDiagram at 0x1796f7e80>])),\n",
       " ('model_218',\n",
       "  (0.3247600197792053, [<ruben.PersistenceDiagram at 0x1799d2bc0>])),\n",
       " ('model_542',\n",
       "  (0.1672000288963318, [<ruben.PersistenceDiagram at 0x17988c4c0>])),\n",
       " ('model_543',\n",
       "  (0.16740000247955322, [<ruben.PersistenceDiagram at 0x17988c4f0>])),\n",
       " ('model_219',\n",
       "  (0.31616002321243286, [<ruben.PersistenceDiagram at 0x179a95480>])),\n",
       " ('model_21',\n",
       "  (0.2628600001335144, [<ruben.PersistenceDiagram at 0x179c1a260>])),\n",
       " ('model_153',\n",
       "  (0.27903997898101807, [<ruben.PersistenceDiagram at 0x179c2dde0>])),\n",
       " ('model_157',\n",
       "  (0.24998003244400024, [<ruben.PersistenceDiagram at 0x179d9e3e0>])),\n",
       " ('model_221',\n",
       "  (0.2555399537086487, [<ruben.PersistenceDiagram at 0x179ec0220>])),\n",
       " ('model_25',\n",
       "  (0.2765600085258484, [<ruben.PersistenceDiagram at 0x179f9fb20>])),\n",
       " ('model_31',\n",
       "  (0.257099986076355, [<ruben.PersistenceDiagram at 0x179fb5480>])),\n",
       " ('model_30',\n",
       "  (0.2638000249862671, [<ruben.PersistenceDiagram at 0x17a155a80>])),\n",
       " ('model_24',\n",
       "  (0.28283995389938354, [<ruben.PersistenceDiagram at 0x17a2a5de0>])),\n",
       " ('model_220',\n",
       "  (0.2657199501991272, [<ruben.PersistenceDiagram at 0x17a44d5a0>])),\n",
       " ('model_156',\n",
       "  (0.25095999240875244, [<ruben.PersistenceDiagram at 0x17a3b13c0>])),\n",
       " ('model_154',\n",
       "  (0.31845998764038086, [<ruben.PersistenceDiagram at 0x17a3b1450>])),\n",
       " ('model_222',\n",
       "  (0.28082001209259033, [<ruben.PersistenceDiagram at 0x17a5a5ba0>])),\n",
       " ('model_26',\n",
       "  (0.3172000050544739, [<ruben.PersistenceDiagram at 0x17a6c78e0>])),\n",
       " ('model_27',\n",
       "  (0.3140000104904175, [<ruben.PersistenceDiagram at 0x17a8b4820>])),\n",
       " ('model_223',\n",
       "  (0.2788999676704407, [<ruben.PersistenceDiagram at 0x17a9d75e0>])),\n",
       " ('model_155',\n",
       "  (0.3206999897956848, [<ruben.PersistenceDiagram at 0x17a8e88e0>]))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pds_and_generalization_gaps(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TASK_1_pds_and_generalization_gaps():\n",
    "    return get_pds_and_generalization_gaps(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TASK_2_pds_and_generalization_gaps():\n",
    "    return get_pds_and_generalization_gaps(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_test(gen_x_pds, train_size=0.7):\n",
    "    training_elems = int(math.floor(train_size*len(gen_x_pds)))\n",
    "    return gen_x_pds[:training_elems], gen_x_pds[training_elems:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_train_and_test(gen_x_pds):\n",
    "    shuffled_gen_x_pds = shuffle(gen_x_pds)\n",
    "    return split_train_and_test(shuffled_gen_x_pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_x_gen_task1 = get_TASK_1_pds_and_generalization_gaps()\n",
    "# pds_x_gen_task2 = get_TASK_2_pds_and_generalization_gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_linear_model():\n",
    "    return LinearRegression()\n",
    "def ridge_linear_model():\n",
    "    return linear_model.Ridge(alpha=.5)\n",
    "def lasso_linear_model():\n",
    "    return linear_model.Lasso(alpha=0.1)\n",
    "def elasticNet():\n",
    "    return ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for extracting features\n",
    "\n",
    "See the other notebooks to see detailed descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruben import PersistenceDiagram\n",
    "# from model.TopologicalSummaries.PersistenceDiagramSummaries import average_life, average_midlife\n",
    "class AllDims:\n",
    "    def __contains__(self, _): return True\n",
    "\n",
    "\n",
    "all_dims = AllDims() # cannot be static method\n",
    "\n",
    "def is_collection(x):\n",
    "    return hasattr(x,\"__contains__\")\n",
    "\n",
    "def alpha_weighted_average_life(persistence_diagram: PersistenceDiagram, alpha: float, dimensions=all_dims):\n",
    "    if not is_collection(dimensions): dimensions = [dimensions]\n",
    "    points = filter(lambda point: point.dim in dimensions, persistence_diagram.points)\n",
    "\n",
    "    life = lambda point: point.death - point.birth\n",
    "    values = map(lambda point: life(point)*life(point)**alpha, points)\n",
    "\n",
    "    val_without_inf = filter(lambda val: not np.isinf(val), values)\n",
    "    return np.mean(np.fromiter(val_without_inf, dtype=np.double))\n",
    "\n",
    "def alpha_weighted_average_midlife(persistence_diagram: PersistenceDiagram, alpha: float, dimensions=all_dims):\n",
    "    if not is_collection(dimensions): dimensions = [dimensions]\n",
    "    points = filter(lambda point: point.dim in dimensions, persistence_diagram.points)\n",
    "\n",
    "    life = lambda point: point.death - point.birth\n",
    "    midlife = lambda point: (point.death + point.birth)/2\n",
    "    values = map(lambda point: midlife(point)*life(point)**alpha, points)\n",
    "\n",
    "    val_without_inf = filter(lambda val: not np.isinf(val), values)\n",
    "    return np.mean(np.fromiter(val_without_inf, dtype=np.double))\n",
    "\n",
    "def average_life(persistence_diagram: PersistenceDiagram, dimensions=all_dims):\n",
    "    return alpha_weighted_average_life(persistence_diagram, 0, dimensions=dimensions)\n",
    "\n",
    "def average_midlife(persistence_diagram: PersistenceDiagram, dimensions=all_dims):\n",
    "    return alpha_weighted_average_midlife(persistence_diagram, 0, dimensions=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooling_vector(persistence_diagram, dims, num_elems=10):\n",
    "        finite_points_pd = persistence_diagram.get_finite_points()\n",
    "        pooling_vector = []\n",
    "        for dim in dims:\n",
    "            points_of_dim = filter(lambda point: point.dim == dim, finite_points_pd)\n",
    "            persistences = list(map(lambda point: point.death - point.birth, points_of_dim))\n",
    "            persistences.sort(reverse=True)\n",
    "            pooling_vector.extend(persistences[:num_elems])\n",
    "            #If we don't have enough persistences we fill the vector with 0s\n",
    "            if(len(persistences) < num_elems):\n",
    "                pooling_vector.extend([0]*(num_elems - len(persistences)))\n",
    "        return np.array(pooling_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_life_midlife_vector(persistence_diagram, dims):\n",
    "    lifes_midlifes = []\n",
    "    for dim in dims:\n",
    "        lifes_midlifes.append(average_life(persistence_diagram, dimensions=[dim]))\n",
    "        lifes_midlifes.append(average_midlife(persistence_diagram, dimensions=[dim]))\n",
    "    lifes_midlifes.append(average_life(persistence_diagram))\n",
    "    lifes_midlifes.append(average_midlife(persistence_diagram))\n",
    "    return np.nan_to_num(np.array(lifes_midlifes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_life_midlife_vector_squared(persistence_diagram, dims):\n",
    "    lifes_midlifes = []\n",
    "    for dim in dims:\n",
    "        lifes_midlifes.append(average_life(persistence_diagram, dimensions=[dim]))\n",
    "        lifes_midlifes.append(average_midlife(persistence_diagram, dimensions=[dim]))\n",
    "    lifes_midlifes.append(average_life(persistence_diagram))\n",
    "    lifes_midlifes.append(average_midlife(persistence_diagram))\n",
    "    life_midlife_array = np.nan_to_num(np.array(lifes_midlifes))\n",
    "    return np.hstack([life_midlife_array, life_midlife_array**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_birth_death(points):\n",
    "    birth_death_pairs = np.array(list(map(lambda point: [point.birth, point.death], points)))\n",
    "    if birth_death_pairs.shape[0] == 0:\n",
    "        return [0, 0]\n",
    "    return np.average(birth_death_pairs, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    return np.array(birth_eath_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_squared(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_death_vector = np.array(birth_eath_averages)\n",
    "    return np.hstack([birth_death_vector, birth_death_vector**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_inverted(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_death = np.array(birth_eath_averages)\n",
    "    return np.hstack([np.log(birth_death+1), birth_death, np.nan_to_num(np.reciprocal(birth_death, out=np.zeros_like(birth_death), where=birth_death!=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_life_midlife_vector(persistence_diagram, dims):\n",
    "    birth_death = get_average_birth_death_vector(persistence_diagram, dims)\n",
    "    life_midlife = get_life_midlife_vector(persistence_diagram, dims)\n",
    "    return np.hstack([birth_death**2, life_midlife**2, birth_death, life_midlife])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(persistence_diagram, dims):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    pe = PersistenceEntropy()\n",
    "    pe_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            pe_vector.append(0)\n",
    "        else:\n",
    "            pe_vector.append(pe.fit_transform(points_3d)[0][0])\n",
    "    return np.array(pe_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_points(persistence_diagram, dims):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    numPoints = NumberOfPoints()\n",
    "    numPoints_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            numPoints_vector.append(0)\n",
    "        else:\n",
    "            numPoints_vector.append(numPoints.fit_transform(points_3d)[0][0])\n",
    "    return np.array(numPoints_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persistence_image_vector(persistence_diagram, dims, n_bins=100):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    pi = PersistenceImage(sigma=0.1, n_bins=n_bins)\n",
    "    pi_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            pi_vector.append(np.zeros(shape=(n_bins,n_bins)))\n",
    "        else:\n",
    "            pi_vector.append(pi.fit_transform(points_3d)[0][0])\n",
    "    return np.array(pi_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_and_std_birth_and_death(points):\n",
    "    if len(points) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    births = np.array(list(map(lambda point: point.birth, points)))\n",
    "    deaths = np.array(list(map(lambda point: point.death, points)))\n",
    "    return [np.std(births), np.std(deaths), np.mean(births), np.mean(deaths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_and_std_birth_and_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    return np.array(birth_eath_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_birth_death_vector_squared(persistence_diagram, dims):\n",
    "    birth_eath_averages = []\n",
    "    finite_points_pd = persistence_diagram.get_finite_points()\n",
    "    for dim in dims:\n",
    "        points_of_dim = list(filter(lambda point: point.dim == dim, finite_points_pd))\n",
    "        birth_eath_averages.extend(average_and_std_birth_and_death(points_of_dim))\n",
    "    birth_eath_averages.extend(average_birth_death(points_of_dim))\n",
    "    birth_eath_averages_squared = np.array(birth_eath_averages)\n",
    "    return np.hstack([birth_eath_averages_squared, birth_eath_averages_squared**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_roots(persistence_diagram, dims, n_coefficients=10, pol_type='T'):\n",
    "    points = persistence_diagram.get_finite_points()\n",
    "    complex_pol = ComplexPolynomial(n_coefficients=n_coefficients, polynomial_type=pol_type)\n",
    "    complex_pol_vector = []\n",
    "    for dim in dims:\n",
    "        dim_points = filter(lambda point: point.dim == dim, points)\n",
    "        points_3d = np.array([list(map(lambda point: [point.birth, point.death, point.dim], dim_points))])\n",
    "        if(points_3d.shape[1] == 0):\n",
    "            complex_pol_vector.extend([0]*(2*n_coefficients))\n",
    "        else:\n",
    "            complex_pol_vector.extend(complex_pol.fit_transform(points_3d)[0])\n",
    "    return np.array(complex_pol_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un dict con todas las features para todos los modelos de task 1 y task 2 y guardarlo en memoria para manipularlo, luego hacer estudios, que ya veremos cuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def bootstraping_function(X, pd_to_vec_fn, dims, samples_per_try=20, tries=5): #Change to 20, 5\n",
    "    def bootstraping_elem(X_elem):\n",
    "        vec_values_per_try = []\n",
    "        for try_it in range(tries):\n",
    "            try_sample = []\n",
    "            for sample_it in range(samples_per_try):\n",
    "                choice = random.randint(0, len(X_elem) - 1)\n",
    "                try_sample.append(pd_to_vec_fn(X_elem[choice], dims))\n",
    "            vec_values_per_try.append(np.mean(np.array(try_sample), axis=0))\n",
    "        return np.mean(np.vstack(vec_values_per_try), axis=0)\n",
    "    return list(map(bootstraping_elem, tqdm(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstraped_features(pds_x_gens, pd_to_vec_fn):\n",
    "    working_dims=[('dim0', [0]), ('dim1', [1]), ('dim01',[0,1])]\n",
    "    def get_pds_and_gen(pds_gen):\n",
    "        _, gen_x_pds = list(zip(*pds_gen))\n",
    "        return list(zip(*gen_x_pds))\n",
    "    features_x_dims = dict()\n",
    "    y, X_raw = get_pds_and_gen(pds_x_gens)\n",
    "    for hashstr, dims in working_dims:\n",
    "        X_feature = bootstraping_function(X_raw, pd_to_vec_fn, dims)\n",
    "        features_x_dims[hashstr] = X_feature\n",
    "    return y, features_x_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Per each feature and task we return a dictionary with the features computed at dims == key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:26<00:00,  1.11it/s]\n",
      "100%|██████████| 96/96 [01:29<00:00,  1.07it/s]\n",
      "100%|██████████| 96/96 [01:35<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "y_values, pooling_task1 = get_bootstraped_features(pds_x_gen_task1, get_pooling_vector)\n",
    "# y_values, pooling_task2 = get_bootstraped_features(pds_x_gen_task2, get_pooling_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [11:33<00:00,  7.22s/it]\n",
      "100%|██████████| 96/96 [12:46<00:00,  7.99s/it]\n",
      "100%|██████████| 96/96 [17:23<00:00, 10.87s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, life_midlife_task1 = get_bootstraped_features(pds_x_gen_task1, get_life_midlife_vector)\n",
    "#y_values, life_midlife_task2 = get_bootstraped_features(pds_x_gen_task2, get_life_midlife_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [12:15<00:00,  7.66s/it]\n",
      "100%|██████████| 96/96 [12:35<00:00,  7.86s/it]\n",
      "100%|██████████| 96/96 [14:27<00:00,  9.04s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, life_midlife_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_life_midlife_vector_squared)\n",
    "#y_values, life_midlife_squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_life_midlife_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:34<00:00,  1.02it/s]\n",
      "100%|██████████| 96/96 [01:41<00:00,  1.05s/it]\n",
      "100%|██████████| 96/96 [01:49<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector)\n",
    "#y_values, average_birth_death_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:34<00:00,  1.02it/s]\n",
      "100%|██████████| 96/96 [01:43<00:00,  1.08s/it]\n",
      "100%|██████████| 96/96 [01:51<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_squared)\n",
    "#y_values, average_birth_death_squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:39<00:00,  1.03s/it]\n",
      "100%|██████████| 96/96 [01:51<00:00,  1.16s/it]\n",
      "100%|██████████| 96/96 [02:04<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_inverted_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_inverted)\n",
    "#y_values, average_birth_death_inverted_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [12:01<00:00,  7.51s/it]\n",
      "100%|██████████| 96/96 [12:30<00:00,  7.82s/it]\n",
      "100%|██████████| 96/96 [15:56<00:00,  9.96s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_birth_death_life_midlife_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_life_midlife_vector)\n",
    "#y_values, average_birth_death_life_midlife_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_life_midlife_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [02:00<00:00,  1.25s/it]\n",
      "100%|██████████| 96/96 [02:19<00:00,  1.46s/it]\n",
      "100%|██████████| 96/96 [02:57<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, entropy_task1 = get_bootstraped_features(pds_x_gen_task1, get_entropy)\n",
    "#y_values, entropy_task2 = get_bootstraped_features(pds_x_gen_task2, get_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [02:15<00:00,  1.41s/it]\n",
      "100%|██████████| 96/96 [02:16<00:00,  1.43s/it]\n",
      "100%|██████████| 96/96 [02:44<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, number_of_points_task1 = get_bootstraped_features(pds_x_gen_task1, get_number_of_points)\n",
    "#y_values, number_of_points_task2 = get_bootstraped_features(pds_x_gen_task2, get_number_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:52<00:00,  1.17s/it]\n",
      "100%|██████████| 96/96 [01:57<00:00,  1.23s/it]\n",
      "100%|██████████| 96/96 [02:25<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_std_birth_death_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector)\n",
    "#y_values, average_std_birth_death_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:47<00:00,  1.12s/it]\n",
      "100%|██████████| 96/96 [01:57<00:00,  1.22s/it]\n",
      "100%|██████████| 96/96 [02:05<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, average_std_birth_death_squared_task1 = get_bootstraped_features(pds_x_gen_task1, get_average_birth_death_vector_squared)\n",
    "#y_values, average_std_birth_death_Squared_task2 = get_bootstraped_features(pds_x_gen_task2, get_average_birth_death_vector_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [27:36<00:00, 17.26s/it]   \n",
      "100%|██████████| 96/96 [18:16<00:00, 11.42s/it]\n",
      "100%|██████████| 96/96 [28:23<00:00, 17.75s/it]\n"
     ]
    }
   ],
   "source": [
    "y_values, complex_roots_task1 = get_bootstraped_features(pds_x_gen_task1, get_complex_roots)\n",
    "#y_values, complex_roots_task2 = get_bootstraped_features(pds_x_gen_task2, get_complex_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1 = dict()\n",
    "# all_features_task2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['pooling'] = pooling_task1\n",
    "# all_features_task2['pooling'] = pooling_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['life_midlife'] = life_midlife_task1\n",
    "# all_features_task2['life_midlife'] = life_midlife_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['life_midlife_squared'] = life_midlife_squared_task1\n",
    "#all_features_task2['life_midlife_squared'] = life_midlife_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death'] = average_birth_death_task1\n",
    "#all_features_task2['average_birth_death'] = average_birth_death_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_squared'] = average_birth_death_squared_task1\n",
    "#all_features_task2['average_birth_death_squared'] = average_birth_death_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_inverted'] = average_birth_death_inverted_task1\n",
    "#all_features_task2['average_birth_death_inverted'] = average_birth_death_inverted_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_birth_death_life_midlife'] = average_birth_death_life_midlife_task1\n",
    "#all_features_task2['average_birth_death_life_midlife'] = average_birth_death_life_midlife_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['entropy'] = entropy_task1\n",
    "#all_features_task2['entropy'] = entropy_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['number_of_points'] = number_of_points_task1\n",
    "#all_features_task2['number_of_points'] = number_of_points_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_std_birth_death'] = average_std_birth_death_task1\n",
    "#all_features_task2['average_std_birth_death'] = average_std_birth_death_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['average_std_birth_death_squared'] = average_std_birth_death_squared_task1\n",
    "#all_features_task2['average_std_birth_death_squared'] = average_std_birth_death_squared_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_task1['complex_roots'] = complex_roots_task1\n",
    "#all_features_task2['complex_roots'] = complex_roots_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(outdir / '../../all_features_task1.pickle', 'wb') as handle1:\n",
    "    pickle.dump(all_features_task1, handle1, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('all_features_task2.pickle', 'wb') as handle2:\n",
    "#     pickle.dump(all_features_task2, handle2, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start working on mixed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir / '../../all_features_task1.pickle', 'rb') as handle1:\n",
    "    all_features_task1 = pickle.load(handle1)\n",
    "\n",
    "# with open('all_features_task2.pickle', 'rb') as handle2:\n",
    "#     all_features_task2 = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, it starts the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_linear_model():\n",
    "    return LinearRegression()\n",
    "def ridge_linear_model():\n",
    "    return linear_model.Ridge(alpha=.5)\n",
    "def lasso_linear_model():\n",
    "    return linear_model.Lasso(alpha=0.1)\n",
    "def elasticNet():\n",
    "    return ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate y train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_values(pds_x_gens):\n",
    "    def get_pds_and_gen(pds_gen):\n",
    "        _, gen_x_pds = list(zip(*pds_gen))\n",
    "        return list(zip(*gen_x_pds))\n",
    "    features_x_dims = dict()\n",
    "    y, _ = get_pds_and_gen(pds_x_gens)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_task1 = get_y_values(pds_x_gen_task1)\n",
    "# y_task2 = get_y_values(pds_x_gen_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_combinations_without_empty_set(all_features):\n",
    "    possible_combinations = list()\n",
    "    keys = all_features.keys()\n",
    "    for cardinality in range(1, len(keys) + 1):\n",
    "        possible_combinations.extend(combinations(keys, cardinality))\n",
    "    return possible_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(all_features, y, possible_dimensions=['dim0', 'dim1', 'dim01'], test_size=0.3, create_model_fn=classic_linear_model, number_of_experiments=10):\n",
    "    results = dict()\n",
    "    possible_combinations = get_all_possible_combinations_without_empty_set(all_features)\n",
    "    for possible_combination in tqdm(possible_combinations):\n",
    "        features_to_train = list()\n",
    "        for feature in possible_combination:\n",
    "            for possible_dimension in possible_dimensions:\n",
    "                features_to_train.append(all_features[feature][possible_dimension])\n",
    "        X = np.hstack(features_to_train)\n",
    "        scores = list()\n",
    "        for _ in range(number_of_experiments):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "            reg = create_model_fn().fit(X_train, y_train)\n",
    "            scores.append(reg.score(X_test, y_test))\n",
    "        results[tuple(possible_combination)] = np.mean(scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4095/4095 [03:14<00:00, 21.05it/s]\n"
     ]
    }
   ],
   "source": [
    "results_task1 = generate_experiment(all_features_task1, y_task1, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_task2 = generate_experiment(all_features_task2, y_task2, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('life_midlife',\n",
       "  'life_midlife_squared',\n",
       "  'average_birth_death',\n",
       "  'entropy',\n",
       "  'number_of_points',\n",
       "  'average_std_birth_death'),\n",
       " 0.5799500384492617)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task1.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('life_midlife_squared',\n",
       "  'average_birth_death',\n",
       "  'average_birth_death_squared',\n",
       "  'entropy',\n",
       "  'average_std_birth_death_squared'),\n",
       " 0.9009350185767977)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task2.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_combinations_without_empty_set_all(selected_features_keys, possible_dimensions=['dim0', 'dim1', 'dim01']):\n",
    "    possible_combinations = list()\n",
    "    keys = list()\n",
    "    for key_raw in selected_features_keys:\n",
    "        for possible_dim in possible_dimensions:\n",
    "            keys.append((key_raw, possible_dim))\n",
    "    for cardinality in range(1, len(keys) + 1):\n",
    "        possible_combinations.extend(combinations(keys, cardinality))\n",
    "    return possible_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_all(all_features, selected_features_keys, y, possible_dimensions=['dim0', 'dim1', 'dim01'], test_size=0.3, create_model_fn=classic_linear_model, number_of_experiments=10):\n",
    "    results = dict()\n",
    "    possible_combinations = get_all_possible_combinations_without_empty_set_all(selected_features_keys, possible_dimensions=possible_dimensions)\n",
    "    for possible_combination in possible_combinations:\n",
    "        features_to_train = list()\n",
    "        for feature, dim in possible_combination:\n",
    "            features_to_train.append(all_features[feature][dim])\n",
    "        X = np.hstack(features_to_train)\n",
    "        scores = list()\n",
    "        try:\n",
    "            for _ in range(number_of_experiments):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "                reg = create_model_fn().fit(X_train, y_train)\n",
    "                scores.append(reg.score(X_test, y_test))\n",
    "            results[tuple(possible_combination)] = np.mean(scores)\n",
    "            \n",
    "        except:\n",
    "            print('The next combination cannot be trained with a linear model')\n",
    "            print(possible_combination)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task1 = generate_experiment_all(all_features_task1, (\n",
    "    'life_midlife',\n",
    "  'life_midlife_squared',\n",
    "    'average_birth_death',\n",
    "  'entropy',\n",
    "    'average_std_birth_death',\n",
    "  'average_std_birth_death_squared'), y_task1, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task2 = generate_experiment_all(all_features_task2, (\n",
    "    'life_midlife',\n",
    "  'life_midlife_squared',\n",
    "    'average_birth_death',\n",
    "  'entropy',\n",
    "    'average_std_birth_death',\n",
    "  'average_std_birth_death_squared'), y_task2, create_model_fn=classic_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('life_midlife_squared', 'dim0'),\n",
       "  ('average_std_birth_death', 'dim01'),\n",
       "  ('average_std_birth_death_squared', 'dim0')),\n",
       " 0.9243755781111016)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task1.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('entropy', 'dim0'),\n",
       "  ('entropy', 'dim1'),\n",
       "  ('average_std_birth_death', 'dim01'),\n",
       "  ('average_std_birth_death_squared', 'dim0')),\n",
       " 0.9230907791163198)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(results_task2.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "da64f636dadf952de2ebe4aa0072f72cf4c362dd113af031c1ed8ef45d32abdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
